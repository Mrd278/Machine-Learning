{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Automated Frontend Designer.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"7SpI1HAfjXQa","colab_type":"code","outputId":"228b2bf9-59a5-4e30-81c0-4578580001e7","executionInfo":{"status":"ok","timestamp":1570613365449,"user_tz":-330,"elapsed":4033,"user":{"displayName":"Codeholic Mridul","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mArTrYw96iMT7Ktw519k5yE3WJFbXgoJEGI4lLNYg=s64","userId":"07783814938878709970"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DOOfqrlrjR4q","colab_type":"code","outputId":"ffc995b5-3ab5-49fe-dbd8-389b9cb69360","executionInfo":{"status":"ok","timestamp":1570613367393,"user_tz":-330,"elapsed":4148,"user":{"displayName":"Codeholic Mridul","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mArTrYw96iMT7Ktw519k5yE3WJFbXgoJEGI4lLNYg=s64","userId":"07783814938878709970"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["cd My\\ Drive"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/gdrive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ysD-gVF8jnKi","colab_type":"code","outputId":"e2061b16-8651-4ac3-cf2a-d37668547773","executionInfo":{"status":"ok","timestamp":1570613367405,"user_tz":-330,"elapsed":3721,"user":{"displayName":"Codeholic Mridul","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mArTrYw96iMT7Ktw519k5yE3WJFbXgoJEGI4lLNYg=s64","userId":"07783814938878709970"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["cd Colab\\ Notebooks"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/gdrive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hFmGQEDHkD2j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"3ca1dba1-aa83-44d1-a3e4-b83dd284436e","executionInfo":{"status":"ok","timestamp":1570613386300,"user_tz":-330,"elapsed":1079,"user":{"displayName":"Codeholic Mridul","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mArTrYw96iMT7Ktw519k5yE3WJFbXgoJEGI4lLNYg=s64","userId":"07783814938878709970"}}},"source":["cd TextVisARd-master/"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/gdrive/My Drive/Colab Notebooks/TextVisARd-master\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZepONKme2ley","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":305},"outputId":"07a916e3-bec4-4d70-8fb1-7e07ea4c3083","executionInfo":{"status":"error","timestamp":1570613474458,"user_tz":-330,"elapsed":3596,"user":{"displayName":"Codeholic Mridul","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mArTrYw96iMT7Ktw519k5yE3WJFbXgoJEGI4lLNYg=s64","userId":"07783814938878709970"}}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"final.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1JLQIiC0vN_LrivyVBMtl2HJxv5IQtAVo\n","\"\"\"\n","\n","from selenium import webdriver\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import requests\n","import urllib.request\n","import time\n","\n","import pandas as pd \n","import numpy as np\n","import matplotlib as plt\n","import pandas as pd\n","import nltk\n","import re\n","import numpy as np\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","from string import punctuation\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from sklearn.feature_extraction.text import CountVectorizer\n","import re\n","\n","#nltk.download()\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","import os\n","root_path = os.getcwd()\n","root_path\n","\n","import os\n","glove_dir = 'glove'\n","embeddings_index = {} # empty dictionary\n","f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","from nltk.corpus import stopwords\n","from nltk.cluster.util import cosine_distance\n","\n","def read(data):\n","    file_name = open(data, \"r\")\n","    file = file_name.readlines()\n","#     print(file)\n","    for i in range(len(file)):\n","        text= file[i].split(\". \")\n","        sentences = []\n","#         for i,sent in text:\n","        for sentence in text:\n","            sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n","#             print(sentence)\n","        sentences.pop() \n","    \n","    return sentences\n","\n","def vectorizer(summ_data,embeddings_index):\n","    summ_vec={}\n","    count=1\n","    for list_w in summ_data:\n","        temp=[]\n","        for word in list_w:\n","            if word in embeddings_index.keys():\n","                temp.append(embeddings_index[word])\n","        summ_vec[count]=temp\n","        count=count+1\n","    return summ_vec\n","\n","def word_binder_dict(vec):\n","    summ_sent_vec={}\n","    for num,list_w in vec.items():\n","        n=1\n","        temp=np.zeros(200)\n","        for word in list_w:\n","            temp=temp+word\n","            n=n+1\n","        summ_sent_vec[num]=(temp/n)\n","    return summ_sent_vec\n","\n","def doc_binder(vec):\n","    temp=np.zeros(200)\n","    n=1\n","    for num,list_w in vec.items():\n","        temp=temp+list_w\n","        n=n+1\n","    doc_vec=(temp/n)\n","        \n","    return doc_vec\n","\n","def dist_bw_sent_doc_norm2(vec1,vec2):\n","    dist_arr={}\n","    for num,sent in vec1.items():\n","        dist = np.linalg.norm(vec2-sent)\n","        dist_arr[num]=dist\n","    return dist_arr\n","\n","def dist_bw_sent_doc_cos(vec1,vec2):\n","    dist_arr={}\n","    for num,sent in vec1.items():\n","        dist =cosine_distance(vec2,sent)\n","        dist_arr[num]=dist\n","    return dist_arr\n","\n","def summarizer(doc_norm2_dist,joined_summ):\n","    summarize_text = []\n","    meaner=[]\n","    for i in range(1,len(doc_norm2_dist)+1):\n","        meaner.append(doc_norm2_dist[i])\n","    mean_arr=np.array(meaner)\n","    threshold=((mean_arr.mean()+np.median(mean_arr))/2)\n","    for i in range(1,len(doc_norm2_dist)+1):\n","        if doc_norm2_dist[i]<threshold:\n","             summarize_text.append(\"\".join(joined_summ[i]))\n","    return summarize_text\n","\n","# stop_words = set(stopwords.words(\"english\"))\n","\n","nltk.download('wordnet')\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","def corpus_maker(summarize_text):\n","    corpus = []\n","    for i in range(len(summarize_text)):\n","        #Remove punctuations\n","        text = re.sub('[^a-zA-Z]', ' ',summarize_text[i] )\n","\n","        #Convert to lowercase\n","        text = text.lower()\n","\n","        #remove tags\n","        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n","\n","        # remove special characters and digits\n","        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n","\n","        ##Convert to list from string\n","        text = text.split()\n","\n","        ##Stemming\n","        ps=PorterStemmer()\n","        #Lemmatisation\n","        lem = WordNetLemmatizer()\n","        text = [lem.lemmatize(word) for word in text if not word in  \n","                stop_words] \n","        text = \" \".join(text)\n","        corpus.append(text)\n","    return corpus\n","\n","def get_top_n_words(corpus, n=None):\n","    vec = CountVectorizer().fit(corpus)\n","    bag_of_words = vec.transform(corpus)\n","    sum_words = bag_of_words.sum(axis=0) \n","    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n","                   vec.vocabulary_.items()]\n","    words_freq =sorted(words_freq, key = lambda x: x[1], \n","                       reverse=True)\n","    return words_freq[:n]\n","\n","def get_top_n2_words(corpus, n=None):\n","    vec1 = CountVectorizer(ngram_range=(2,2),  \n","            max_features=2000).fit(corpus)\n","    bag_of_words = vec1.transform(corpus)\n","    sum_words = bag_of_words.sum(axis=0) \n","    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n","                  vec1.vocabulary_.items()]\n","    words_freq =sorted(words_freq, key = lambda x: x[1], \n","                reverse=True)\n","    return words_freq[:n]\n","\n","def get_top_n3_words(corpus, n=None):\n","    vec1 = CountVectorizer(ngram_range=(3,3), \n","           max_features=2000).fit(corpus)\n","    bag_of_words = vec1.transform(corpus)\n","    sum_words = bag_of_words.sum(axis=0) \n","    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n","                  vec1.vocabulary_.items()]\n","    words_freq =sorted(words_freq, key = lambda x: x[1], \n","                reverse=True)\n","    return words_freq[:n]\n","\n","for i in range(0,1):\n","    summ_data=read(\"halo.txt\")\n","    joined_summ={}\n","    n=0\n","    \n","    for list_w in summ_data:\n","        n=n+1\n","        text=' '.join(list_w)\n","        joined_summ[n]=text\n","        \n","    summ_vec=vectorizer(summ_data,embeddings_index)\n","    summ_sent_vec=word_binder_dict(summ_vec)\n","    doc_vec=doc_binder(summ_sent_vec)\n","    doc_cos_dist=dist_bw_sent_doc_cos(summ_sent_vec,doc_vec)\n","    doc_norm2_dist=dist_bw_sent_doc_norm2(summ_sent_vec,doc_vec)\n","    \n","    summarize_text=summarizer(doc_norm2_dist,joined_summ)\n","    with open('summary.txt', 'w') as f:\n","        for item in summarize_text:\n","            f.write(\"%s\\n\" % item)\n","    \n","    stop_words = set(stopwords.words(\"english\"))\n","        \n","    corpus=corpus_maker(summarize_text)\n","    cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n","    X=cv.fit_transform(corpus)\n","    \n","    n=5\n","    \n","    top_words = get_top_n_words(corpus, n)\n","    top_df = pd.DataFrame(top_words)\n","    top_df.columns=[\"Word\", \"Freq\"]\n","    \n","    uni=[]\n","    \n","    for word in top_df['Word']:\n","        uni.append(word)\n","    with open('purang_dega.txt', 'w') as f:\n","        for item in uni:\n","            f.write(\"%s\\n\" % item)\n","    \n","    top2_words = get_top_n2_words(corpus, n)\n","    top2_df = pd.DataFrame(top2_words)\n","    top2_df.columns=[\"Bi-gram\", \"Freq\"]\n","    top2_df['Bi-gram']\n","    \n","    bi=[]\n","    \n","    for word in top2_df['Bi-gram']:\n","        bi.append(word)\n","    with open('purang_dega.txt', 'w') as f:\n","        for item in bi:\n","            f.write(\"%s\\n\" % item)\n","   \n","    top3_words = get_top_n3_words(corpus, n)\n","    top3_df = pd.DataFrame(top3_words)\n","    top3_df.columns=[\"Tri-gram\", \"Freq\"]\n","    \n","    tri=[]\n","    \n","    for word in top3_df['Tri-gram']:\n","        tri.append(word)\n","    with open('purang_dega.txt', 'w') as f:\n","        for item in tri:\n","            f.write(\"%s\\n\" % item)\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-924b12b65ecd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mglove_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glove'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# empty dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove.6B.200d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove/glove.6B.200d.txt'"]}]},{"cell_type":"code","metadata":{"id":"tzJr7z072x-T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"outputId":"3144ff0d-4ce1-470e-ce91-2c8b7d9d9e1d","executionInfo":{"status":"ok","timestamp":1570613467299,"user_tz":-330,"elapsed":5885,"user":{"displayName":"Codeholic Mridul","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mArTrYw96iMT7Ktw519k5yE3WJFbXgoJEGI4lLNYg=s64","userId":"07783814938878709970"}}},"source":["!pip install selenium"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Collecting selenium\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n","\u001b[K     |████████████████████████████████| 911kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n","Installing collected packages: selenium\n","Successfully installed selenium-3.141.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ejQC4aLS24FM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}